{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-18362367aac2>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-18362367aac2>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    scrapy crawl aliexpress_tablets\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AliexpressTabletsSpider(scrapy.Spider):\n",
    "    name = 'aliexpress_tablets'\n",
    "    allowed_domains = ['aliexpress.com']\n",
    "    start_urls = ['https://www.aliexpress.com/category/200216607/tablets.html',\n",
    "                 'https://www.aliexpress.com/category/200216607/tablets/2.html?site=glo&g=y&tag=']\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        print(\"procesing:\"+response.url)\n",
    "        #Extract data using css selectors\n",
    "        product_name=response.css('.product::text').extract()\n",
    "        price_range=response.css('.value::text').extract()\n",
    "        #Extract data using xpath\n",
    "        orders=response.xpath(\"//em[@title='Total Orders']/text()\").extract()\n",
    "        company_name=response.xpath(\"//a[@class='store $p4pLog']/text()\").extract()\n",
    "\n",
    "        row_data=zip(product_name,price_range,orders,company_name)\n",
    "\n",
    "        #Making extracted data row wise\n",
    "        for item in row_data:\n",
    "            #create a dictionary to store the scraped info\n",
    "            scraped_info = {\n",
    "                #key:value\n",
    "                'page':response.url,\n",
    "                'product_name' : item[0], #item[0] means product in the list and so on, index tells what value to assign\n",
    "                'price_range' : item[1],\n",
    "                'orders' : item[2],\n",
    "                'company_name' : item[3],\n",
    "            }\n",
    "\n",
    "            #yield or give the scraped info to scrapy\n",
    "            yield scraped_info\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\service_identity\\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n",
      "2019-04-20 14:53:49 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-04-20 14:53:49 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0\n",
      "2019-04-20 14:53:49 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-04-20 14:53:49 [scrapy.extensions.telnet] INFO: Telnet Password: 4df4521c54456d63\n",
      "2019-04-20 14:53:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-04-20 14:53:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-04-20 14:53:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-04-20 14:53:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-04-20 14:53:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-04-20 14:53:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-04-20 14:53:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6043\n",
      "2019-04-20 14:53:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'From The Creators Of Scrapy: Artificial Intelligence Data Extraction API'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Scrapinghub’s New AI Powered Developer Data Extraction API for E-Commerce & Article Extraction'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Solution Architecture Part 2: How to Define The Scope of Your Web Scraping Project'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'How to Architect a Web Scraping Solution: The Step-by-Step Guide'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Navigating Compliance\\xa0When Extracting Web Scraped Alternative Financial Data'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'St Patrick’s Day Special: Finding Dublin’s Best Pint of Guinness With Web Scraping'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Spidermon: Scrapinghub’s Secret Sauce To Our Data Quality & Reliability Guarantee'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Meet Spidermon: Scrapinghub’s Battle Tested Spider Monitoring Library [Now Open Sourced]'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'Proxy Management: Should I Build My Proxy Infrastructure In-House Or Use A Off-The-Shelf Proxy Solution?'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
      "{'title': 'A Sneak Peek Inside Crawlera: The World’s Smartest Web Scraping Proxy Network'}\n",
      "2019-04-20 14:53:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/2> (referer: https://blog.scrapinghub.com)\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'Why We Created Crawlera? The World’s Smartest Web Scraping Proxy Network'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'The Rise of Web Data in Hedge Fund Decision Making & The Importance of Data Quality'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'The Predictive Power of Web Scraped Product Data For Institutional Investors: A GoPro Case Study'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'The Challenges E-Commerce Retailers Face Managing Their Web Scraping Proxies'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'Looking Back at 2018'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'Do What is Right Not What is Easy!'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'Shubber GetTogether 2018'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'Data Quality Assurance for Enterprise Web Scraping'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'What I Learned as a Google Summer of Code student at Scrapinghub'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/2>\n",
      "{'title': 'GDPR Compliance For Web Scrapers: The Step-By-Step Guide'}\n",
      "2019-04-20 14:53:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/3> (referer: https://blog.scrapinghub.com/page/2)\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'For E-Commerce Data Scientists: Lessons Learned Scraping 100 Billion Products Pages'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'A Sneak Peek Inside What Hedge Funds Think of Alternative Financial Data'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Want to Predict Fitbit’s Quarterly Revenue? Eagle Alpha Did It Using Web Scraped Product Data'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'How Data Compliance Companies Are Turning To Web Crawlers To Take Advantage of the GDPR Business Opportunity'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Looking Back at 2017'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'A Faster, Updated Scrapinghub'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Scraping the Steam Game Store with Scrapy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Do Androids Dream of Electric Sheep?'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Deploy your Scrapy Spiders from GitHub'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/3>\n",
      "{'title': 'Looking Back at 2016'}\n",
      "2019-04-20 14:53:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/4> (referer: https://blog.scrapinghub.com/page/3)\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'How to Increase Sales with Online Reputation Management'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'How to Build your own Price Monitoring Tool'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'How You Can Use Web Data to Accelerate Your Startup'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'An Introduction to XPath: How to Get Started'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'Why Promoting Open Data Increases Economic Opportunities'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'Interview: How Up Hail uses Scrapy to Increase Transparency'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'How to Run Python Scripts in Scrapy Cloud'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'Embracing the Future of Work: How To Communicate Remotely'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'How to Deploy Custom Docker Images for Your Web Crawlers'}\n",
      "2019-04-20 14:53:50 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/4>\n",
      "{'title': 'Improved Frontera: Web Crawling at Scale with Python 3 Support'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/5> (referer: https://blog.scrapinghub.com/page/4)\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'How to Crawl the Web Politely with Scrapy'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Introducing Scrapy Cloud with Python 3 Support'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'What the Suicide Squad Tells Us About Web Data'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'This Month in Open Source at Scrapinghub August 2016'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Meet Parsel: the Selector Library behind Scrapy'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Incremental Crawls with Scrapy and DeltaFetch'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Improving Access to Peruvian Congress Bills with Scrapy'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Scrapely: The Brains Behind Portia Spiders'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Introducing Portia2Code: Portia Projects into Scrapy Spiders'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/5>\n",
      "{'title': 'Scraping Infinite Scrolling Pages'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/6> (referer: https://blog.scrapinghub.com/page/5)\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'This Month in Open Source at Scrapinghub June 2016'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Introducing the Datasets Catalog'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Introducing the Crawlera Dashboard'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Data Extraction with Scrapy and Python 3'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'How to Debug your Scrapy Spiders'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Scrapy + MonkeyLearn: Textual Analysis of Web Data'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Introducing Scrapy Cloud 2.0'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'A (not so) Short Story on Getting Decent Internet Access'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Scraping Websites Based on ViewStates with Scrapy'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/6>\n",
      "{'title': 'Machine Learning with Web Scraping: New MonkeyLearn Addon'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/7> (referer: https://blog.scrapinghub.com/page/6)\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Mapping Corruption in the Panama Papers with Open Data'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Web Scraping to Create Open Data'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Scrapy Tips from the Pros: March 2016 Edition'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'This Month in Open Source at Scrapinghub March 2016'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Join Scrapinghub for Google Summer of Code 2016'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'How Web Scraping is Revealing Lobbying and Corruption in Peru'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Splash 2.0 Is Here with Qt 5 and Python 3'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Migrate your Kimono Projects to Portia'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Scrapy Tips from the Pros: February 2016 Edition'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/7>\n",
      "{'title': 'Portia: The Open Source Alternative to Kimono Labs'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/8> (referer: https://blog.scrapinghub.com/page/7)\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Web Scraping Finds Stores Guilty of Price Inflation'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Python 3 is Coming to Scrapy'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Happy Anniversary: Scrapinghub Turns 5'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Scrapy Tips from the Pros: Part 1'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Vizlegal: Rise of Machine-Readable Laws and Court Judgments'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': \"Christmas Eve vs New Year's Eve: Last Minute Price Inflation?\"}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Looking Back at 2015'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Winter Sales Showdown: Black Friday vs Cyber Monday vs Green Monday'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Chats With RINAR Solutions'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/8>\n",
      "{'title': 'Black Friday, Cyber Monday: Are They Worth It?'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/9> (referer: https://blog.scrapinghub.com/page/8)\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Tips for Creating a Cohesive Company Culture Remotely'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Parse Natural Language Dates with Dateparser'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Aduana: Link Analysis to Crawl the Web at Scale'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Scrapy on the Road to Python 3 Support'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Introducing Javascript support for Portia'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'Distributed Frontera: Web Crawling at Scale'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'The Road to Loading JavaScript in Portia'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'EuroPython 2015'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'StartupChats Remote Working Q&A'}\n",
      "2019-04-20 14:53:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/9>\n",
      "{'title': 'PyCon Philippines 2015'}\n",
      "2019-04-20 14:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/10> (referer: https://blog.scrapinghub.com/page/9)\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Google Summer of Code 2015'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Link Analysis Algorithms Explained'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'EuroPython, here we go!'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Aduana: Link Analysis with Frontera'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Using git to manage vacations in a large distributed team'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Gender Inequality Across Programming Languages'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Traveling Tips for Remote Workers'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'A Career in Remote Working'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Frontera: The Brain Behind the Crawls'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/10>\n",
      "{'title': 'Scrape Data Visually with Portia and Scrapy Cloud'}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/11> (referer: https://blog.scrapinghub.com/page/10)\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Scrapinghub: A Remote Working Success Story'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Why we moved to Slack'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'The History of Scrapinghub'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Skinfer: A Tool for Inferring JSON Schemas'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Handling JavaScript in Scrapy with Splash'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Scrapinghub Crawls the Deep Web'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'New Changes to Our Scrapy Cloud Platform'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Introducing ScrapyRT: An API for Scrapy spiders'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'Looking Back at 2014'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/11>\n",
      "{'title': 'XPath Tips from the Web Scraping Trenches'}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/12> (referer: https://blog.scrapinghub.com/page/11)\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Introducing Data Reviews'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Extracting schema.org Microdata Using Scrapy Selectors and XPath'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Announcing Portia, the Open Source Visual Web Scraper!'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Optimizing Memory Usage of Scikit-Learn Models Using Succinct Tries'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Open Source at Scrapinghub'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Looking Back at 2013'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Marcos Campal Is a ScrapingHubber!'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Introducing Dash'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Why MongoDB Is a Bad Choice for Storing Our Scraped Data'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/12>\n",
      "{'title': 'Introducing Crawlera, a Smart Page Downloader'}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/13> (referer: https://blog.scrapinghub.com/page/12)\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Git Workflow for Scrapy Projects'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'How to Fill Login Forms Automatically'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Spiders activity graphs'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Finding Similar Items'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Scrapy 0.15 dropping support for Python 2.5'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Autoscraping casts a wider net'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Scrapy 0.14 released'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Dirbot - a new example Scrapy project'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Introducing w3lib and scrapely'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/13>\n",
      "{'title': 'Scrapy 0.12 released'}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com/page/14> (referer: https://blog.scrapinghub.com/page/13)\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/14>\n",
      "{'title': 'Spoofing your Scrapy bot IP using tsocks'}\n",
      "2019-04-20 14:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com/page/14>\n",
      "{'title': 'Hello, world'}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-04-20 14:53:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5580,\n",
      " 'downloader/request_count': 14,\n",
      " 'downloader/request_method_count/GET': 14,\n",
      " 'downloader/response_bytes': 143172,\n",
      " 'downloader/response_count': 14,\n",
      " 'downloader/response_status_count/200': 14,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 4, 20, 11, 53, 52, 877193),\n",
      " 'item_scraped_count': 132,\n",
      " 'log_count/DEBUG': 147,\n",
      " 'log_count/INFO': 8,\n",
      " 'request_depth_max': 13,\n",
      " 'response_received_count': 14,\n",
      " 'scheduler/dequeued': 14,\n",
      " 'scheduler/dequeued/memory': 14,\n",
      " 'scheduler/enqueued': 14,\n",
      " 'scheduler/enqueued/memory': 14,\n",
      " 'start_time': datetime.datetime(2019, 4, 20, 11, 53, 49, 702495)}\n",
      "2019-04-20 14:53:52 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ec4238f726df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBlogSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the script will block here until the crawling is finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a.next-posts-link'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('.post-header>h2'):\n",
    "            yield {'title': title.css('a ::text').get()}\n",
    "\n",
    "        for next_page in response.css('a.next-posts-link'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    })\n",
    "\n",
    "    process.crawl(BlogSpider)\n",
    "    process.start() # the script will block here until the crawling is finished\n",
    "print(response.css('a.next-posts-link'))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\service_identity\\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n",
      "2019-04-20 15:50:12 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-04-20 15:50:12 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Windows-10-10.0.17134-SP0\n",
      "2019-04-20 15:50:12 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-04-20 15:50:12 [scrapy.extensions.telnet] INFO: Telnet Password: f8c9159131c4546f\n",
      "2019-04-20 15:50:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-04-20 15:50:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-04-20 15:50:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-04-20 15:50:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-04-20 15:50:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-04-20 15:50:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-04-20 15:50:13 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6043\n",
      "2019-04-20 15:50:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://lizaalert.org/forum/viewforum.php?f=287> (referer: None)\n",
      "2019-04-20 15:50:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://lizaalert.org/forum/viewforum.php?f=287> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-3-1b018a59745b>\", line 16, in parse_front\n",
      "    course_blocks = response.css('//a[contains(@class,\"right-box right\")]')\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\scrapy\\http\\response\\text.py\", line 122, in css\n",
      "    return self.selector.css(query)\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 262, in css\n",
      "    return self.xpath(self._css2xpath(query))\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\parsel\\selector.py\", line 265, in _css2xpath\n",
      "    return self._csstranslator.css_to_xpath(query)\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\parsel\\csstranslator.py\", line 109, in css_to_xpath\n",
      "    return super(HTMLTranslator, self).css_to_xpath(css, prefix)\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\cssselect\\xpath.py\", line 192, in css_to_xpath\n",
      "    for selector in parse(css))\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\cssselect\\parser.py\", line 355, in parse\n",
      "    return list(parse_selector_group(stream))\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\cssselect\\parser.py\", line 368, in parse_selector_group\n",
      "    yield Selector(*parse_selector(stream))\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\cssselect\\parser.py\", line 376, in parse_selector\n",
      "    result, pseudo_element = parse_simple_selector(stream)\n",
      "  File \"C:\\Users\\nikzo\\Anaconda3\\lib\\site-packages\\cssselect\\parser.py\", line 475, in parse_simple_selector\n",
      "    \"Expected selector, got %s\" % (peek,))\n",
      "  File \"<string>\", line None\n",
      "cssselect.parser.SelectorSyntaxError: Expected selector, got <DELIM '/' at 0>\n",
      "2019-04-20 15:50:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-04-20 15:50:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 237,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 16936,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 4, 20, 12, 50, 14, 44322),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 8,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/SelectorSyntaxError': 1,\n",
      " 'start_time': datetime.datetime(2019, 4, 20, 12, 50, 13, 415996)}\n",
      "2019-04-20 15:50:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = \"https://lizaalert.org/forum/viewforum.php?f=287\",\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('//a[contains(@class,\"right-box right\")]')\n",
    "    course_links = course_blocks.xpath('./@href')\n",
    "    links_to_follow = \"\\\"https://lizaalert.org/forum\" + str(hrefs_from_css.extract()[0])[1:] + \"\\\"\"\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = links_to_follow,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('a.topictitle')\n",
    "    return crs_title\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.xpath('a.topictitle./@href')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "print(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALOOOOOOOOOOOO ['2', '0']\n",
      "20\n",
      "[<Selector xpath='//a[contains(@class,\"right-box right\")]' data='<a href=\"./viewforum.php?f=202&amp;sid=6'>]\n",
      "<Selector xpath='descendant-or-self::*/@href' data='./viewforum.php?f=202&sid=66f66f66d7e268'>]\n",
      "ссылка= https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=25\n",
      "\"https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=25\"\n",
      "/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=\n",
      "['/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=', '/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=']\n",
      "[25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375, 400, 425, 450, 475]\n",
      "<class 'str'>\n",
      "https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=275\n",
      "['https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=25', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=50', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=75', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=100', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=125', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=150', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=175', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=200', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=225', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=250', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=275', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=300', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=325', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=350', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=375', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=400', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=425', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=450', 'https://lizaalert.org/forum/viewforum.php?f=202&sid=66f66f66d7e2687add9185f89d733267&start=475']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import bs4\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from scrapy import Selector\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get('https://lizaalert.org/forum/viewforum.php?f=202').content #url='google.com'\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "\n",
    "uClient = uReq('https://lizaalert.org/forum/viewforum.php?f=202')\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    "containers2 = page_soup.findAll(\"div\", {\"class\":\"pagination\"})\n",
    "containers22 = str(containers2)\n",
    "#print(containers22.find(\"Страница 1 из \"))\n",
    "#print(containers22)\n",
    "\n",
    "def page12(page1, containers22):\n",
    "    page1 = list()\n",
    "    c = containers22.find(\"из \")\n",
    "    page1.append(containers22[c+11])\n",
    "    page1.append(containers22[c+12])\n",
    "    return page1\n",
    "\n",
    "print('ALOOOOOOOOOOOO', page12(page1,containers22))\n",
    "page_real = int(page12(page1,containers22)[0] + page12(page1,containers22)[1])\n",
    "print(page_real)\n",
    "\n",
    "# Парсим\n",
    "course_as = sel.xpath('//a[contains(@class,\"right-box right\")]')\n",
    "print(course_as)\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "print(str(hrefs_from_css)[1:])\n",
    "next1 = \"https://lizaalert.org/forum\" + str(hrefs_from_css.extract()[0])[1:]\n",
    "print(\"ссылка=\", next1)\n",
    "links_to_follow = \"\\\"https://lizaalert.org/forum\" + str(hrefs_from_css.extract()[0])[1:] + \"\\\"\"\n",
    "print(links_to_follow)\n",
    "\n",
    "clean_link = str(hrefs_from_css.extract()[0])[1:len(hrefs_from_css.extract()[0])-2]\n",
    "print(clean_link)\n",
    "list_links = list()\n",
    "for i in range((page_real)-1):\n",
    "    list_links.append(clean_link)\n",
    "print(list_links)\n",
    "\n",
    "#За 500 надо пояснить! Но это вроде несложно\n",
    "i=0\n",
    "list1 = list()\n",
    "for i in range(0,(page_real-1)*25, 25):\n",
    "    i = i + 25\n",
    "    list1.append(i)\n",
    "print(list1)\n",
    "\n",
    "result = [str(item) for item in list1]\n",
    "\n",
    "print(type(result[2]))\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list_links[i] += result[i]\n",
    "#print(list_links)\n",
    "\n",
    "for i in range(len(list_links)):\n",
    "    list_links[i] = \"https://lizaalert.org/forum\" + list_links[i]\n",
    "print(list_links[10])\n",
    "print(list_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lizaalert.org/forum/viewtopic.php?f=202&t=6802&sid=66f66f66d7e2687add9185f89d733267\n",
      "492\n"
     ]
    }
   ],
   "source": [
    "list_full = list()\n",
    "url = \"'\" + list_links[5] + \"'\"\n",
    "url1 = list_links[7]\n",
    "for i in range((page_real)-1):\n",
    "    html = requests.get(list_links[i]).content #url='google.com'\n",
    "    sel = Selector(text=html)\n",
    "\n",
    "    course_as1 = sel.css('a.topictitle')\n",
    "    hrefs1_from_css = course_as1.css('::attr(href)')\n",
    "#syka\n",
    "    list0 = list()\n",
    "    for i in range(0, len(hrefs1_from_css.extract())):\n",
    "        list0.append(\"https://lizaalert.org/forum\" + str(hrefs1_from_css.extract()[i])[1:])\n",
    "        list_full.append(list0[i])\n",
    "print(list_full[485])\n",
    "print(len(list_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
